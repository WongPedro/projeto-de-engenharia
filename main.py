# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xldWouE89bo7a8Jon8DVxEdNoQUi-7Bg

$$η = \mbox{learning_rate} \\
q = \mbox{sampling_probabilities}\\
K = \mbox{client_data}\\
E = $$
"""

import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from random import sample
import hashlib

def get_random_clients(client_data:list, q:list)-> dict:

    clients = choices(client_data, k=len(client_data))
    clients_data_dict = {}

    for i, client in enumerate(clients):
        data = str(client).encode()
        clients_data_dict.update({
            hashlib.sha256(data).hexdigest(): {"prob":q[i], "value": client}
        })
        print(hashlib.sha256(data).hexdigest())
    q_is = []
    count = {}
    client_list = []
    for client in clients:
        data = str(client).encode()
        key = hashlib.sha256(data).hexdigest()
        print(key)
        if key in count:
            print("entrei no if")
            count[key] += 1
        else:
            print("entrei no else")
            count[key] = 1
            q_is.append(clients_data_dict[key].get("prob"))

    for client in list(count.keys()):
        client_list.append(clients_data_dict[key].get("value"))

    return client_list, np.array(list(count.values()))/sum(np.array(list(count.values()))),q_is

def federated_sgd(sampling_probabilities, client_data, E, w_0, learning_rate, precision):

    weights = np.copy(w_0)

    previous_weights = np.copy(weights)
    random_clients = get_random_clients(client_data,q)
    k = len(client_data)
    round = 0

    while True:
        # Each client
        for client_id,_ in enumerate(random_clients[0]):
            # Select client data
            data = random_clients[0][client_id]
            print(len(data))

            weights_client = np.copy(weights)
            print(len(weights_client))
            # Train locally for a specified number of epochs
            for i in range(E):
                # Compute predictions
                predictions = np.dot(data, weights_client)

                # Update weights using predictions (unsupervised learning)
                errors = predictions - np.mean(predictions, axis=0)  # Center the predictions
                gradient = np.dot(data.T, errors) / len(data)

                # Update weights client
                weights_client -= learning_rate * gradient

            p_i = random_clients[1][client_id]
            q_i = random_clients[2][client_id]
            weights += ((p_i)/(k*q_i))*(weights_client - previous_weights)  #p-i = n/#k
        round += 1
        # Check convergence
        print(np.linalg.norm(weights - previous_weights))
        if np.linalg.norm(weights - previous_weights) < precision:
            break
        previous_weights = np.copy(weights)
        if round > 10:
            print(round)
            break

    return weights, round

"""- Trocar para modelo uniforme (1/n de probabilidade de ser sorteado)
- $p_i$ = quantidade de amostras do cliente i/ total de amostras
"""

import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# Carregar o conjunto de dados MNIST
mnist = fetch_openml('mnist_784', version=1, cache=True)

# Dividir o conjunto de dados em características e rótulos
X, y = mnist["data"], mnist["target"]

# Normalizar os valores dos pixels para o intervalo [0, 1]
X = MinMaxScaler().fit_transform(X)

# Dividir o conjunto de dados em conjunto de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Dividir o conjunto de treinamento em "clientes"
num_clients = 5
client_data = []
client_labels = []

for i in range(num_clients):
    start = i * (len(X_train) // num_clients)
    end = (i + 1) * (len(X_train) // num_clients)
    client_data.append(X_train[start:end])
    client_labels.append(y_train[start:end])

import time
def federated_sgd_v2(client_data, k, e, w_0, learning_rate, precision):

    weights_norm = []
    weights = np.copy(w_0)
    previous_weights = np.copy(weights)

    n = len(client_data)
    q = 1/len(client_data) #uniform #TODO colocar sem ser uniforme
    round = 0
    random_clients = sample(client_data,k)
    total_sample = sum([len(clients) for clients in random_clients])
    inicio = time.time()
    while True:
        # Each client
        for client_id,_ in enumerate(random_clients):

            data = random_clients[client_id]
            weights_client = np.copy(weights)

            for i in range(e):

                predictions = np.dot(data, weights_client)
                errors = predictions - np.mean(predictions, axis=0)  # Center the predictions
                gradient = np.dot(data.T, errors) / len(data)
                # Update weights client
                weights_client -= learning_rate * gradient

            p_i = len(random_clients[client_id])/total_sample
            weights += ((p_i)/(k*q))*(weights_client - previous_weights)  #p-i = n/#k
        round += 1

        # Check convergence
        weights_norm.append(np.linalg.norm(weights - previous_weights))
        # print(np.linalg.norm(weights - previous_weights))
        if np.linalg.norm(weights - previous_weights) < precision:
            break
        previous_weights = np.copy(weights)

        # if round > 100:
        #     break

    fim = time.time()
    wall_clock_time = fim - inicio

    return weights, round, weights_norm, wall_clock_time

# Inicializar os pesos do modelo
num_features = X_train.shape[1]  # Adicionando um viés
initial_weights = np.random.randn(num_features)

# Parâmetros de treinamento
learning_rate = 0.1
# precision = 0.0001 #TODO ver esse valor depois
precision = 0.05
num_local_epochs = 20
q = (np.zeros(num_clients) + 1)/ num_clients

# Treinando o modelo usando a função federated_sgd
# final_weights = federated_sgd_v2(client_data, 5, num_local_epochs, initial_weights, learning_rate, precision)

# print("Pesos finais:", final_weights[0])
# print("Rodadas:", final_weights[1])

rs = []
ws = []
w_ns = []
w_c_ts = []
for i in range(2,5):
    print(f"Inicio do treinamento com {i+1} clientes")
    w, r, w_n, w_c_t = federated_sgd_v2(client_data, i+1, num_local_epochs, initial_weights, learning_rate, precision)
    print(f"Fim do treinamento com {i+1} clientes")
    ws.append(w)
    rs.append(r)
    w_ns.append(w_n)
    w_c_ts.append(w_c_t)

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
lines = ["-", "--", ":"]
# plt.style.use('default')
for n, w_n in enumerate(w_ns):
    xs = [k+1 for k,_ in enumerate(w_n)]
    plt.plot(xs, w_n, label=f'qt de clientes = {n+3}', linestyle = lines[n])

plt.xlabel('Rodadas')
plt.ylabel('Norma de w')
plt.title('Gráfico de Convergência')
plt.legend()
plt.grid(True)
plt.show()

number_clients = [3, 4, 5]
plt.scatter(rs[0], w_c_ts[0], label=f'qt de clientes = {3}')
plt.scatter(rs[1], w_c_ts[1], label=f'qt de clientes = {4}')
plt.scatter(rs[2], w_c_ts[2], label=f'qt de clientes = {5}')
plt.ylabel('Tempo total de convergencia(s)')
plt.xlabel('Quantidade de rodadas')
plt.legend()
plt.grid()

"""Fazer:
 - Procurar um dataset que nós conhecemos a distribuição/ que nós conseguissemos confirmar resultado;
 - Tentar entender essa relação linear (se é do dataset ou não)
 - Começar pensar já na modelagem matematica.
 - Ajusta o SGD para receber clientes com diferentes quantidades de dados.
"""

